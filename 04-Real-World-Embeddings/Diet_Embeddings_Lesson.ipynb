{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "5046af33",
            "metadata": {},
            "source": [
                "# Real World Embeddings: Diet & Food ðŸ¥—\n",
                "\n",
                "Welcome to the **Real World**! \n",
                "In the previous lesson, we used a \"Toy Model\" with 3 simple dimensions (Fruit, Dog, Cat). \n",
                "Now, we will use a powerful, pre-trained AI model to understand complex sentences about food and diet.\n",
                "\n",
                "**What we will do:**\n",
                "1.  **Setup**: Prepare our tools and environment.\n",
                "2.  **Embed**: Turn 15 diet-related sentences into \"Vectors\" (lists of numbers).\n",
                "3.  **Store**: Save these vectors into **Qdrant** (our Vector Database).\n",
                "4.  **Search**: Ask a question like \"I want to build muscle\" and find the most relevant sentences."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3558d2db",
            "metadata": {},
            "source": [
                "## 1. Setup & Environment\n",
                "\n",
                "### Why do we need a Virtual Environment?\n",
                "It is best practice to keep your project dependencies isolated. This prevents conflicts between different projects (e.g., Project A needs `numpy` 1.0, Project B needs `numpy` 2.0).\n",
                "\n",
                "Run the cell below to create a virtual environment `.venv`. \n",
                "**Note:** After running this, if you are in VS Code, you should select this new `.venv` as your Kernel/Interpreter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "93ba460a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a virtual environment named .venv\n",
                "!python3 -m venv .venv\n",
                "\n",
                "# Note: To use this, you must select '.venv' as your Kernel in VS Code (Top Right Corner)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "17eca156",
            "metadata": {},
            "source": [
                "### Install Libraries\n",
                "We need two main libraries:\n",
                "1.  `sentence-transformers`: The AI library that turns text into vectors.\n",
                "2.  `qdrant-client`: The tool that lets us talk to the Qdrant database.\n",
                "\n",
                "We also install `pandas` to help us visualize data nicely."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9024d2c9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting sentence-transformers\n",
                        "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
                        "Collecting qdrant-client\n",
                        "  Downloading qdrant_client-1.16.2-py3-none-any.whl.metadata (11 kB)\n",
                        "Collecting pandas\n",
                        "  Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
                        "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
                        "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
                        "Collecting tqdm (from sentence-transformers)\n",
                        "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
                        "Collecting torch>=1.11.0 (from sentence-transformers)\n",
                        "  Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
                        "Collecting scikit-learn (from sentence-transformers)\n",
                        "  Downloading scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
                        "Collecting scipy (from sentence-transformers)\n",
                        "  Downloading scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
                        "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
                        "  Downloading huggingface_hub-1.2.4-py3-none-any.whl.metadata (13 kB)\n",
                        "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
                        "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
                        "Collecting filelock (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
                        "  Downloading filelock-3.20.2-py3-none-any.whl.metadata (2.1 kB)\n",
                        "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
                        "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
                        "Collecting numpy>=1.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
                        "  Downloading numpy-2.4.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/raghunandana.sanur/Desktop/test-repo/vector-database-demo-2026/.venv/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
                        "Collecting pyyaml>=5.1 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
                        "  Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
                        "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
                        "  Downloading regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
                        "Collecting requests (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
                        "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
                        "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
                        "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
                        "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
                        "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
                        "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
                        "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
                        "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
                        "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
                        "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
                        "  Using cached grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
                        "Collecting httpx>=0.20.0 (from httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
                        "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
                        "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
                        "Collecting protobuf>=3.20.0 (from qdrant-client)\n",
                        "  Downloading protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
                        "Collecting pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 (from qdrant-client)\n",
                        "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
                        "Collecting urllib3<3,>=1.26.14 (from qdrant-client)\n",
                        "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/raghunandana.sanur/Desktop/test-repo/vector-database-demo-2026/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
                        "Collecting pytz>=2020.1 (from pandas)\n",
                        "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
                        "Collecting tzdata>=2022.7 (from pandas)\n",
                        "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
                        "Collecting anyio (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
                        "Collecting certifi (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
                        "Collecting httpcore==1.* (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
                        "Collecting idna (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
                        "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
                        "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
                        "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
                        "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
                        "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
                        "Collecting annotated-types>=0.6.0 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client)\n",
                        "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
                        "Collecting pydantic-core==2.41.5 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client)\n",
                        "  Downloading pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
                        "Collecting typing-inspection>=0.4.2 (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client)\n",
                        "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
                        "Requirement already satisfied: six>=1.5 in /Users/raghunandana.sanur/Desktop/test-repo/vector-database-demo-2026/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
                        "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
                        "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
                        "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
                        "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
                        "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
                        "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
                        "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
                        "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
                        "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
                        "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
                        "  Downloading markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
                        "Collecting charset_normalizer<4,>=2 (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
                        "  Downloading charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
                        "Collecting joblib>=1.3.0 (from scikit-learn->sentence-transformers)\n",
                        "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
                        "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence-transformers)\n",
                        "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
                        "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
                        "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading qdrant_client-1.16.2-py3-none-any.whl (377 kB)\n",
                        "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
                        "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
                        "Downloading pandas-2.3.3-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
                        "Using cached grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl (11.8 MB)\n",
                        "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
                        "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
                        "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
                        "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
                        "Downloading h2-4.3.0-py3-none-any.whl (61 kB)\n",
                        "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
                        "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
                        "Downloading numpy-2.4.0-cp313-cp313-macosx_14_0_arm64.whl (5.2 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading protobuf-6.33.2-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
                        "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
                        "Downloading pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
                        "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
                        "Downloading pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
                        "Downloading regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
                        "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
                        "Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
                        "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
                        "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
                        "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
                        "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
                        "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
                        "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
                        "Downloading filelock-3.20.2-py3-none-any.whl (16 kB)\n",
                        "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
                        "Downloading markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
                        "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
                        "Downloading charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
                        "Downloading scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl (8.0 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
                        "Downloading scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
                        "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
                        "Installing collected packages: pytz, mpmath, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, sympy, setuptools, safetensors, regex, pyyaml, protobuf, portalocker, numpy, networkx, MarkupSafe, joblib, idna, hyperframe, hpack, hf-xet, h11, fsspec, filelock, charset_normalizer, certifi, annotated-types, typing-inspection, scipy, requests, pydantic-core, pandas, jinja2, httpcore, h2, grpcio, anyio, torch, scikit-learn, pydantic, huggingface-hub, httpx, tokenizers, transformers, qdrant-client, sentence-transformers\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47/47\u001b[0m [sentence-transformers]sformers]]b]\n",
                        "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 annotated-types-0.7.0 anyio-4.12.1 certifi-2026.1.4 charset_normalizer-3.4.4 filelock-3.20.2 fsspec-2025.12.0 grpcio-1.76.0 h11-0.16.0 h2-4.3.0 hf-xet-1.2.0 hpack-4.1.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 hyperframe-6.1.0 idna-3.11 jinja2-3.1.6 joblib-1.5.3 mpmath-1.3.0 networkx-3.6.1 numpy-2.4.0 pandas-2.3.3 portalocker-3.2.0 protobuf-6.33.2 pydantic-2.12.5 pydantic-core-2.41.5 pytz-2025.2 pyyaml-6.0.3 qdrant-client-1.16.2 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.16.3 sentence-transformers-5.2.0 setuptools-80.9.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.2 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 typing-inspection-0.4.2 typing_extensions-4.15.0 tzdata-2025.3 urllib3-2.6.2\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install sentence-transformers qdrant-client pandas"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e157a881",
            "metadata": {},
            "source": [
                "## 2. Load the Embedding Model ðŸ§ \n",
                "\n",
                "We are using a model called `all-MiniLM-L6-v2`.\n",
                "*   **MiniLM**: It's a smaller, faster version of the famous BERT model.\n",
                "*   **L6**: It has 6 layers of \"neurons\".\n",
                "*   **v2**: Version 2.\n",
                "\n",
                "This model has read billions of sentences. It knows that \"King\" is related to \"Queen\", and \"Protein\" is related to \"Muscle\".\n",
                "It converts any text into a list of **384 numbers**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "2fe3a88b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/raghunandana.sanur/Desktop/test-repo/vector-database-demo-2026/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# Download and load the model\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "447a1b3a",
            "metadata": {},
            "source": [
                "## 3. The Data (Our Knowledge Base) ðŸ“š\n",
                "\n",
                "Here represent 15 diverse sentences about food. \n",
                "We have Keto, Vegan, Paleo, Junk Food, and Healthy Eating examples.\n",
                "This list is what we will search through later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "5a42b69e",
            "metadata": {},
            "outputs": [],
            "source": [
                "sentences = [\n",
                "    \"I follow a strict ketogenic diet with high fat and low carbs.\",\n",
                "    \"Veganism is a lifestyle that excludes all animal products.\",\n",
                "    \"I love eating pepperoni pizza implies a lot of cheese.\",\n",
                "    \"Paleo diet focuses on whole foods like meat and vegetables.\",\n",
                "    \"My daily calories come from sugary soda and fast food.\",\n",
                "    \"A plant-based diet is rich in fiber and vitamins.\",\n",
                "    \"I am allergic to gluten so I avoid bread and pasta.\",\n",
                "    \"Intermittent fasting helps me manage my weight.\",\n",
                "    \"Protein shakes are great for muscle recovery after a workout.\",\n",
                "    \"I eat a balanced diet with plenty of fruits and water.\",\n",
                "    \"Fried chicken and french fries are my favorite cheat meal.\",\n",
                "    \"Mediterranean diet involves olive oil and fresh fish.\",\n",
                "    \"Sugar-free diets can help reduce inflammation.\",\n",
                "    \"Carnivore diet means eating only meat and animal products.\",\n",
                "    \"I count my macros to ensure I get enough protein.\"\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f75cccb3",
            "metadata": {},
            "source": [
                "## 4. Transformation (Text -> Numbers) ðŸ”¢\n",
                "\n",
                "We use `model.encode()` to pass our text through the Neural Network.\n",
                "The output `embeddings` is a list of vectors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "2d480026",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 15 vectors.\n",
                        "[[ 0.03987455  0.01822555  0.0602175  ... -0.03542068  0.02106852\n",
                        "   0.02349543]\n",
                        " [ 0.012781    0.0123942   0.01697932 ... -0.08436691  0.08080513\n",
                        "   0.02343985]\n",
                        " [-0.02017428 -0.01320761  0.03650714 ...  0.07151126  0.03967074\n",
                        "  -0.076828  ]\n",
                        " ...\n",
                        " [-0.00504594  0.0077768   0.04835589 ... -0.09266385  0.02395846\n",
                        "  -0.01080788]\n",
                        " [ 0.00324737  0.08738441  0.02378584 ...  0.0184341   0.06495902\n",
                        "   0.04318139]\n",
                        " [-0.00763889 -0.06681007 -0.00521442 ... -0.05119191 -0.06289448\n",
                        "  -0.09797092]]\n",
                        "Shape of one vector: (384,)\n"
                    ]
                }
            ],
            "source": [
                "embeddings = model.encode(sentences)\n",
                "\n",
                "print(f\"Created {len(embeddings)} vectors.\")\n",
                "print(f\"{embeddings}\")\n",
                "print(f\"Shape of one vector: {embeddings[0].shape}\")\n",
                "# 384 dimensions means each sentence is described by 384 features!"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ea821f54",
            "metadata": {},
            "source": [
                "## 5. Store in Qdrant ðŸ’¾\n",
                "\n",
                "Now we have the vectors, but they are just in memory. Let's save them to a Database.\n",
                "\n",
                "1.  **Connect**: We connect to Qdrant running on `localhost:6333`.\n",
                "2.  **Create Collection**: We make a bucket named `diet_facts`. We MUST tell Qdrant the vector size (**384**).\n",
                "3.  **Upsert**: We upload our data. Each item needs:\n",
                "    *   **ID**: A unique number (0, 1, 2...)\n",
                "    *   **Vector**: The 384 numbers we just generated.\n",
                "    *   **Payload**: The original text (so we can read it back)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c69854ed",
            "metadata": {},
            "outputs": [],
            "source": [
                "from qdrant_client import QdrantClient\n",
                "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
                "\n",
                "# 1. Connect\n",
                "client = QdrantClient(url=\"http://localhost:6333\")\n",
                "\n",
                "collection_name = \"diet_facts\"\n",
                "\n",
                "# 2. Create Collection (Refresh if exists)\n",
                "if client.collection_exists(collection_name):\n",
                "    client.delete_collection(collection_name)\n",
                "\n",
                "client.create_collection(\n",
                "    collection_name=collection_name,\n",
                "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
                ")\n",
                "print(f\"Collection '{collection_name}' created!\")\n",
                "\n",
                "# 3. Prepare Data for Upsert\n",
                "points = []\n",
                "for idx, (sentence, vector) in enumerate(zip(sentences, embeddings)):\n",
                "    points.append(PointStruct(\n",
                "        id=idx,\n",
                "        vector=vector.tolist(),\n",
                "        payload={\"text\": sentence}\n",
                "    ))\n",
                "\n",
                "# 4. Upload\n",
                "client.upsert(\n",
                "    collection_name=collection_name,\n",
                "    points=points\n",
                ")\n",
                "print(f\"Successfully uploaded {len(points)} facts to Qdrant.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b8212aba",
            "metadata": {},
            "source": [
                "## 6. Semantic Search ðŸ”Ž\n",
                "\n",
                "This is the magic part.\n",
                "We want to find sentences related to **\"Muscle\"** and **\"Healthy Eating\"**.\n",
                "\n",
                "**The Process:**\n",
                "1.  **Query**: Write a new sentence (Query).\n",
                "2.  **Encode**: Convert that Query into a Vector (using the SAME model).\n",
                "3.  **Search**: Send that Vector to Qdrant. Qdrant finds the closest vectors in the database.\n",
                "4.  **Result**: We get back the most similar facts!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1f79eb94",
            "metadata": {},
            "outputs": [],
            "source": [
                "query_text = \"I want to build muscle and eat healthy\"\n",
                "print(f\"Query: '{query_text}'\")\n",
                "\n",
                "# 1. Encode the Query\n",
                "query_vector = model.encode(query_text)\n",
                "\n",
                "# 2. Search in Qdrant\n",
                "search_results = client.query_points(\n",
                "    collection_name=collection_name,\n",
                "    query=query_vector,\n",
                "    limit=3  # Give me the top 3 best matches\n",
                ").points\n",
                "\n",
                "# 3. Print Results\n",
                "print(\"\\n--- Top 3 Results ---\")\n",
                "for hit in search_results:\n",
                "    print(f\"Score: {hit.score:.4f} | Text: {hit.payload['text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b3248cfb",
            "metadata": {},
            "source": [
                "### Why did it pick these?\n",
                "You should see sentences about **Protein**, **Macros**, and **Plant-based** diets.\n",
                "*   Our query mentioned \"Muscle\". The model knows \"Protein\" is related to Muscle.\n",
                "*   Our query mentioned \"Healthy\". The model knows \"Plant-based\" and \"Macros\" are related to health.\n",
                "*   It likely **IGNORED** the \"Pizza\" and \"Fries\" sentences because they are semantically far away from \"Healthy\"."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
